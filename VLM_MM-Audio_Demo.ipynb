{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313514a9-27ef-48af-95d2-be160fd319dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Please refer to the README.md for any difficulties running this notebook.\n",
    "import gc\n",
    "import time\n",
    "\n",
    "import librosa\n",
    "import numpy as np\n",
    "import requests\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoProcessor,\n",
    "    BitsAndBytesConfig,\n",
    "    GenerationConfig,\n",
    "    Qwen2AudioForConditionalGeneration,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830879d7-6cd4-436e-975b-fa63d75bc1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import re\n",
    "from io import BytesIO\n",
    "\n",
    "from PIL import Image, ImageDraw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13188deb-111a-4e31-99b2-be5d1b655b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import panel as pn\n",
    "\n",
    "pn.extension(\"filedropper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf332d33-03a4-400a-9be6-cc584b7dfe83",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_store = {\n",
    "    \"Loaded\": False,\n",
    "    \"History\": [],\n",
    "    \"Model\": None,\n",
    "    \"Processor\": None,\n",
    "    \"Loaded\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5934087-bbed-4f4b-912a-a6fd5747a77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Update with references to your downloaded model paths\n",
    "model_path = \"/home/example/common/molmo_aria/Molmo-7B-D-0924\"\n",
    "aria_model_path = \"/shared/example/models/Aria\"\n",
    "qwen_audio_model_path = \"/home/example/common/LLMs/Qwen2-Audio-7B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f542c7d-cd0f-491c-8476-b45c129b8afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(event):\n",
    "    global model_store\n",
    "    global model_info_pane\n",
    "    if model_store[\"Model\"]:\n",
    "        model_cleanup()\n",
    "\n",
    "    match toggle_group.value:\n",
    "        case \"Molmo-7B-D-0924\":\n",
    "            model_info_pane.object = f\"<p>Loading {toggle_group.value}...</p>\"\n",
    "            model_store[\"Processor\"] = AutoProcessor.from_pretrained(\n",
    "                model_path,\n",
    "                trust_remote_code=True,\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                device_map=\"auto\",\n",
    "            )\n",
    "            model_store[\"Model\"] = AutoModelForCausalLM.from_pretrained(\n",
    "                model_path,\n",
    "                trust_remote_code=True,\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                device_map=\"auto\",\n",
    "            )\n",
    "            model_info_pane.object = f\"<p>{toggle_group.value} loaded.</p>\"\n",
    "            model_store[\"Loaded\"] = True\n",
    "        case \"Molmo-7B-D-0924-4bit\":\n",
    "            model_info_pane.object = f\"<p>Loading {toggle_group.value}...</p>\"\n",
    "            model_store[\"Processor\"] = AutoProcessor.from_pretrained(\n",
    "                model_path,\n",
    "                trust_remote_code=True,\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                device_map=\"auto\",\n",
    "            )\n",
    "            arguments = {\n",
    "                \"device_map\": \"auto\",\n",
    "                \"torch_dtype\": \"auto\",\n",
    "                \"trust_remote_code\": True,\n",
    "            }\n",
    "            quantization_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_quant_type=\"fp4\",  # or nf4\n",
    "                bnb_4bit_use_double_quant=False,\n",
    "            )\n",
    "            arguments[\"quantization_config\"] = quantization_config\n",
    "            model_store[\"Model\"] = AutoModelForCausalLM.from_pretrained(\n",
    "                model_path, **arguments\n",
    "            )\n",
    "            model_info_pane.object = f\"<p>{toggle_group.value} loaded.</p>\"\n",
    "            model_store[\"Loaded\"] = True\n",
    "        case \"Aria\":\n",
    "            model_info_pane.object = f\"<p>Loading {toggle_group.value}...</p>\"\n",
    "\n",
    "            model_id_or_path = aria_model_path\n",
    "            model_store[\"Processor\"] = AutoProcessor.from_pretrained(\n",
    "                model_id_or_path, trust_remote_code=True\n",
    "            )\n",
    "            model_store[\"Model\"] = AutoModelForCausalLM.from_pretrained(\n",
    "                model_id_or_path,\n",
    "                device_map=\"auto\",\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                trust_remote_code=True,\n",
    "            )\n",
    "\n",
    "            model_info_pane.object = f\"<p>{toggle_group.value} loaded.</p>\"\n",
    "            model_store[\"Loaded\"] = True\n",
    "        case \"Qwen2-Audio\":\n",
    "            model_info_pane.object = f\"<p>Loading {toggle_group.value}...</p>\"\n",
    "\n",
    "            model_id_or_path = qwen_audio_model_path\n",
    "            model_store[\"Processor\"] = AutoProcessor.from_pretrained(model_id_or_path)\n",
    "            model_store[\"Model\"] = Qwen2AudioForConditionalGeneration.from_pretrained(\n",
    "                model_id_or_path, device_map=\"auto\"\n",
    "            )\n",
    "\n",
    "            model_info_pane.object = f\"<p>{toggle_group.value} loaded.</p>\"\n",
    "            model_store[\"Loaded\"] = True\n",
    "        case _:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df18ed7-4566-47f0-a599-35230c079baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_cleanup():\n",
    "    global model_store\n",
    "    global model_info_pane\n",
    "    if model_store[\"Model\"]:\n",
    "        model_info_pane.object = \"<p><b>No Model Loaded</b></p>\"\n",
    "        del model_store[\"Model\"]\n",
    "        del model_store[\"Processor\"]\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        model_store[\"Model\"] = None\n",
    "        model_store[\"Processor\"] = None\n",
    "        model_store[\"Loaded\"] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bc2a4a-0e8e-4f2f-9595-2f49d2519432",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_points(points_str):\n",
    "    # Regex to extract each <points> tag with multiple x and y pairs\n",
    "    point_tags = re.findall(r\"<points (.*?)>(.*?)</points>\", points_str)\n",
    "    if len(point_tags) == 0:\n",
    "        point_tags = re.findall(r\"<point (.*?)>(.*?)</point>\", points_str)\n",
    "    parsed_points = []\n",
    "    if len(point_tags) == 0:\n",
    "        return None\n",
    "\n",
    "    for attributes, label in point_tags:\n",
    "        coordinates = re.findall(r'x\\d+=\"(.*?)\" y\\d+=\"(.*?)\"', attributes)\n",
    "        if not coordinates:\n",
    "            single_coordinate = re.findall(r'x=\"(.*?)\" y=\"(.*?)\"', attributes)\n",
    "            if single_coordinate:\n",
    "                coordinates = [single_coordinate[0]]\n",
    "        parsed_points.append(\n",
    "            {\n",
    "                \"label\": label,\n",
    "                \"coordinates\": [(float(x), float(y)) for x, y in coordinates],\n",
    "            }\n",
    "        )\n",
    "    return parsed_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbeafb2f-ab7b-4982-9fc6-ead22751fef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlay_points(points_data):\n",
    "    global file_dropper\n",
    "    if file_dropper.value:\n",
    "        file_name, file_content = next(iter(file_dropper.value.items()))\n",
    "        image = Image.open(io.BytesIO(file_content))\n",
    "    else:\n",
    "        return\n",
    "\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    width, height = image.size\n",
    "\n",
    "    for point_data in points_data:\n",
    "        label = point_data[\"label\"]\n",
    "        for x_percent, y_percent in point_data[\"coordinates\"]:\n",
    "            x = (x_percent / 100) * width\n",
    "            y = (y_percent / 100) * height\n",
    "            radius = int(height / 55)\n",
    "            draw.ellipse((x - radius, y - radius, x + radius, y + radius), fill=\"blue\")\n",
    "\n",
    "        # Optionally, add label text next to the first coordinate\n",
    "        # if point_data[\"coordinates\"]:\n",
    "        #     x, y = point_data[\"coordinates\"][0]\n",
    "        #     draw.text((x, y - 10), label, fill=\"yellow\")\n",
    "\n",
    "    image_pane.object = image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09caa6a-e9d0-45ff-a246-783fea22f9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image(event):\n",
    "    if file_dropper.value:\n",
    "        if list(file_dropper.mime_type.values())[0].split(\"/\")[0] == \"image\":\n",
    "            audio_pane.object = None\n",
    "            audio_pane.visible = False\n",
    "            file_name, file_content = next(iter(file_dropper.value.items()))\n",
    "            image = Image.open(io.BytesIO(file_content))\n",
    "            image_preview_html.object = \"<p>Scaled Image Preview:</p>\"\n",
    "            image_pane.object = image\n",
    "        elif list(file_dropper.mime_type.values())[0].split(\"/\")[0] == \"audio\":\n",
    "            image_pane.object = None\n",
    "            file_name, file_content = next(iter(file_dropper.value.items()))\n",
    "            image_preview_html.object = \"<p>Audio Track:</p>\"\n",
    "            audio = librosa.load(io.BytesIO(file_content))\n",
    "            audio_pane.sample_rate = sample_rate = audio[1]\n",
    "            audio_pane.object = np.int16(np.array(audio[0], dtype=np.float32) * 32767)\n",
    "            audio_pane.visible = True\n",
    "    else:\n",
    "        image_preview_html.object = \"<p></p>\"\n",
    "        image_pane.object = None\n",
    "        audio_pane.object = None\n",
    "        audio_pane.visible = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d581f938-29cb-469d-bcf3-93d76e734be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_chat_history(chat_interface):\n",
    "    return [{\"role\": i.user, \"content\": i.object} for i in chat_interface.objects]\n",
    "\n",
    "\n",
    "def compile_prompt_gguf(\n",
    "    history,\n",
    "    user_name,\n",
    "    assistant_name,\n",
    "    system_prompt=\"You are an unbiased, helpful assistant.\",\n",
    "):\n",
    "    messages = []\n",
    "    for i in history:\n",
    "        if i[\"role\"] == user_name:\n",
    "            messages.append(\n",
    "                {\"role\": \"user\", \"content\": [{\"text\": i[\"content\"], \"type\": \"text\"}]}\n",
    "            )\n",
    "        elif i[\"role\"] == assistant_name:\n",
    "            messages.append(\n",
    "                {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": [{\"text\": i[\"content\"], \"type\": \"text\"}],\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    if messages[-1][\"role\"] == \"user\":\n",
    "        messages[-1][\"content\"].append({\"text\": None, \"type\": \"image\"})\n",
    "    return messages\n",
    "\n",
    "\n",
    "def compile_prompt(\n",
    "    history,\n",
    "    user_name,\n",
    "    assistant_name,\n",
    "    system_prompt=\"You are an unbiased, helpful assistant.\",\n",
    "):\n",
    "    texts = [f\"\"]\n",
    "    for i in history:\n",
    "        if i[\"role\"] == user_name:\n",
    "            texts.append(f'<|startoftext|>USER: {i[\"content\"]}\\nASSISTANT:')\n",
    "        elif i[\"role\"] == assistant_name:\n",
    "            if i[\"content\"][-13:] == \"<|endoftext|>\":\n",
    "                texts.append(f'{i[\"content\"]}\\n')\n",
    "            elif i[\"content\"][-15:] == \"<|endoftext|>\\n\":\n",
    "                texts.append(f'{i[\"content\"]}')\n",
    "            else:\n",
    "                texts.append(f'{i[\"content\"]}<|endoftext|>\\n')\n",
    "        else:\n",
    "            pass\n",
    "    return \"\".join(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8dbb930-27f3-4433-8e7f-778ae45c50cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "toggle_group = pn.widgets.ToggleGroup(\n",
    "    name=\"Model Select\",\n",
    "    options=[\"Molmo-7B-D-0924\", \"Molmo-7B-D-0924-4bit\", \"Aria\", \"Qwen2-Audio\"],\n",
    "    behavior=\"radio\",\n",
    ")\n",
    "load_button = pn.widgets.Button(name=\"Load Model\", button_type=\"primary\")\n",
    "load_button.on_click(load_model)\n",
    "\n",
    "model_info_pane = pn.pane.HTML(\"<p><b>No Model Loaded</b></p>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb785e3c-6edc-46ea-a9bc-3029493be917",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dropper = pn.widgets.FileDropper(\n",
    "    accepted_filetypes=[\"image/*\", \"audio/*\"],\n",
    "    multiple=False,\n",
    "    max_file_size=\"10MB\",\n",
    "    width=300,\n",
    "    height=95,\n",
    ")\n",
    "\n",
    "image_pane = pn.pane.Image(sizing_mode=\"scale_width\", max_width=550)\n",
    "audio_pane = pn.pane.Audio(sizing_mode=\"scale_width\", max_width=550, visible=False)\n",
    "image_preview_html = pn.pane.HTML(\"<p></p>\")\n",
    "file_dropper.param.watch(display_image, \"value\")\n",
    "\n",
    "image_load = pn.Column(\n",
    "    file_dropper,\n",
    "    pn.Column(\n",
    "        image_preview_html,\n",
    "        audio_pane,\n",
    "        image_pane,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52a6b31-d856-4ed3-bc5b-cc3a2b57730a",
   "metadata": {},
   "outputs": [],
   "source": [
    "left_bar = pn.Column(\n",
    "    toggle_group,\n",
    "    pn.Row(load_button, model_info_pane),\n",
    "    image_load,\n",
    "    width=600,\n",
    "    height=800,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a951c096-1448-4449-a5a7-166a44b5fa4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def callback_vlm(contents: str, user: str, instance: pn.chat.ChatInterface):\n",
    "    global model_store\n",
    "    global file_dropper\n",
    "\n",
    "    if not model_store[\"Loaded\"]:\n",
    "        instance.send(\n",
    "            \"Loading model; one moment please...\",\n",
    "            user=\"System\",\n",
    "            respond=False,\n",
    "        )\n",
    "        load_model(None)\n",
    "        null_and_void = instance.objects.pop()\n",
    "\n",
    "    if toggle_group.value in [\"Molmo-7B-D-0924\", \"Molmo-7B-D-0924-4bit\"]:\n",
    "        if file_dropper.value:\n",
    "            if list(file_dropper.mime_type.values())[0].split(\"/\")[0] == \"image\":\n",
    "                file_name, file_content = next(iter(file_dropper.value.items()))\n",
    "                image = Image.open(io.BytesIO(file_content))\n",
    "        else:\n",
    "            return \"Please upload an image using the file dropper in order to talk over that image.\"\n",
    "\n",
    "        prompt_full = compile_prompt(build_chat_history(instance), \"User\", \"Assistant\")\n",
    "\n",
    "        inputs = model_store[\"Processor\"].process(images=[image], text=prompt_full)\n",
    "\n",
    "        inputs = {\n",
    "            k: v.to(model_store[\"Model\"].device).unsqueeze(0) for k, v in inputs.items()\n",
    "        }\n",
    "\n",
    "        with torch.autocast(device_type=\"cuda\", enabled=True, dtype=torch.bfloat16):\n",
    "            output = model_store[\"Model\"].generate_from_batch(\n",
    "                inputs,\n",
    "                GenerationConfig(max_new_tokens=1250, stop_strings=\"<|endoftext|>\"),\n",
    "                tokenizer=model_store[\"Processor\"].tokenizer,\n",
    "            )\n",
    "\n",
    "        generated_tokens = output[0, inputs[\"input_ids\"].size(1) :]\n",
    "        model_store[\"History\"].append(generated_tokens)\n",
    "        generated_text = model_store[\"Processor\"].tokenizer.decode(\n",
    "            generated_tokens, skip_special_tokens=True\n",
    "        )\n",
    "\n",
    "        points_data = parse_points(generated_text)\n",
    "        if points_data:\n",
    "            overlay_points(points_data)\n",
    "        time.sleep(0.1)\n",
    "        return generated_text\n",
    "    elif toggle_group.value == \"Aria\":\n",
    "        if file_dropper.value:\n",
    "            if list(file_dropper.mime_type.values())[0].split(\"/\")[0] == \"image\":\n",
    "                file_name, file_content = next(iter(file_dropper.value.items()))\n",
    "                image = Image.open(io.BytesIO(file_content))\n",
    "        else:\n",
    "            return \"Please upload an image using the file dropper in order to talk over that image.\"\n",
    "\n",
    "        messages = compile_prompt_gguf(\n",
    "            build_chat_history(instance), \"User\", \"Assistant\"\n",
    "        )\n",
    "        text = model_store[\"Processor\"].apply_chat_template(\n",
    "            messages, add_generation_prompt=True\n",
    "        )\n",
    "        inputs = model_store[\"Processor\"](text=text, images=image, return_tensors=\"pt\")\n",
    "        inputs[\"pixel_values\"] = inputs[\"pixel_values\"].to(model_store[\"Model\"].dtype)\n",
    "        inputs = {k: v.to(model_store[\"Model\"].device) for k, v in inputs.items()}\n",
    "\n",
    "        with torch.inference_mode(), torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
    "            output = model_store[\"Model\"].generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=500,\n",
    "                stop_strings=[\"<|im_end|>\"],\n",
    "                tokenizer=model_store[\"Processor\"].tokenizer,\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "            )\n",
    "            output_ids = output[0][inputs[\"input_ids\"].shape[1] :]\n",
    "            result = model_store[\"Processor\"].decode(\n",
    "                output_ids, skip_special_tokens=True\n",
    "            )\n",
    "            result = result.replace(\"<|im_end|>\", \"\")\n",
    "        time.sleep(0.1)\n",
    "        return result\n",
    "    elif toggle_group.value == \"Qwen2-Audio\":\n",
    "        if file_dropper.value:\n",
    "            if list(file_dropper.mime_type.values())[0].split(\"/\")[0] == \"audio\":\n",
    "                _, audio_file_content = next(iter(file_dropper.value.items()))\n",
    "        else:\n",
    "            return \"Please attach an audio sample of the appropriate file format\"\n",
    "\n",
    "        messages = build_chat_history(instance)[-1]\n",
    "        if messages[\"role\"] == \"User\":\n",
    "            text_input = messages[\"content\"]\n",
    "        else:\n",
    "            return \"Error handling input content - please restart application and try again.\"\n",
    "\n",
    "        conversation = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"audio\", \"audio_url\": \"Filler.wav\"},\n",
    "                    {\"type\": \"text\", \"text\": text_input},\n",
    "                ],\n",
    "            },\n",
    "        ]\n",
    "        text = model_store[\"Processor\"].apply_chat_template(\n",
    "            conversation, add_generation_prompt=True, tokenize=False\n",
    "        )\n",
    "        audios = []\n",
    "        for message in conversation:\n",
    "            if isinstance(message[\"content\"], list):\n",
    "                for ele in message[\"content\"]:\n",
    "                    if ele[\"type\"] == \"audio\":\n",
    "                        try:\n",
    "                            audios.append(\n",
    "                                librosa.load(\n",
    "                                    io.BytesIO(audio_file_content),\n",
    "                                    sr=model_store[\n",
    "                                        \"Processor\"\n",
    "                                    ].feature_extractor.sampling_rate,\n",
    "                                )[0]\n",
    "                            )\n",
    "                        except:\n",
    "                            return \"Error loading audio file, please change file dropper content to appropriate file format\"\n",
    "\n",
    "        inputs = model_store[\"Processor\"](\n",
    "            text=text, audios=audios, return_tensors=\"pt\", padding=True\n",
    "        )\n",
    "        inputs.input_ids = inputs.input_ids.to(\"cuda\")\n",
    "        inputs[\"input_ids\"] = inputs[\"input_ids\"].to(\"cuda\")\n",
    "\n",
    "        generate_ids = model_store[\"Model\"].generate(**inputs, max_length=256)\n",
    "        generate_ids = generate_ids[:, inputs.input_ids.size(1) :]\n",
    "\n",
    "        response = model_store[\"Processor\"].batch_decode(\n",
    "            generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "        )[0]\n",
    "        time.sleep(0.1)\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4a9130-6603-44de-a0bf-916b31bff016",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"main.html\", \"r\") as f:\n",
    "    header_html = f.read().replace(\"\\n\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0c63ba-d7a3-4859-bc66-c30de2fe1baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "header_pane = pn.pane.HTML(header_html, width_policy=\"max\", sizing_mode=\"stretch_width\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cd7929-35d4-4ecb-8e68-67688a9edea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_interface = pn.chat.ChatInterface(\n",
    "    callback=callback_vlm, callback_exception=\"verbose\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8621340-959f-4f03-8420-471166ec739b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.Column(header_pane, pn.Row(left_bar, chat_interface)).servable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4022bd61-6d75-4706-bcd2-b9d6e2eb6b7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
